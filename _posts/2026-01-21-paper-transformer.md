---
title: "Paper Review: Attention Is All You Need"
date: 2026-01-22 10:00:00 +0900
categories: [PaperReview]
tags: [transformer, attention]
---

## Paper Information
- Authors: Vaswani et al.
- Conference: NeurIPS 2017

## Motivation
The paper proposes a novel architecture that removes recurrence
and convolution entirely.

## Method
The model is based solely on self-attention mechanisms.

## Key Contributions
- Transformer architecture
- Scaled dot-product attention
- Multi-head attention

## Personal Notes
This architecture significantly influenced modern NLP models.
